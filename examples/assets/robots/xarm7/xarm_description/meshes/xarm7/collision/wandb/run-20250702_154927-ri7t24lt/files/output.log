GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type           | Params
-----------------------------------------
0 | model | SequentialFlow | 3.2 M
-----------------------------------------
3.2 M     Trainable params
0         Non-trainable params
3.2 M     Total params
12.936    Total estimated model params size (MB)
/home/ps/anaconda3/envs/node_ik/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/ps/py_project/nodeik/examples/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Validation sanity check:   0%|                                                                   | 0/1 [00:00<?, ?it/s]Module warp.sim.articulation load took 0.36 ms
/home/ps/anaconda3/envs/node_ik/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/ps/anaconda3/envs/node_ik/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:227: UserWarning: You called `self.log('test_set_size', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/ps/anaconda3/envs/node_ik/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:227: UserWarning: You called `self.log('samples_per_endpoint', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
Epoch 50:   2%| | 8/501 [00:07<08:01,  1.02it/s, loss=-0.747, v_num=24lt, train_loss_step=0.0874, val_loss=-0.794, trai
/home/ps/anaconda3/envs/node_ik/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
                                                                                                                       
